/*
 * SPDX-License-Identifier: MIT
 * Copyright (c) Huawei Technologies Co., Ltd. 2025-2025. All rights reserved.
 * Description: add fused_deep_moe file
 * Create: 2025-12-10
 * Note:
 * History: 2025-12-10 fused_deep_moe file
 */

#include <unistd.h>
#include <hccl/hccl.h>
#include <torch/extension.h>
#include <torch/csrc/autograd/custom_function.h>
#include "torch_npu/csrc/core/npu/NPUStream.h"
#include "pytorch_npu_helper.hpp"
#include <hccl/hccl.h>
#include <iostream>

using torch::autograd::AutogradContext;
using torch::autograd::Function;
using TensorVector = std::vector<at::Tensor>;
using namespace at;
using namespace std;


std::vector<at::Tensor> FusedDeepMoeImplNpu(
    const at::Tensor &x, \
    const at::Tensor &expertIds, \
    const at::TensorList &gmm1PermutedWeight, \
    const at::TensorList &gmm1PermutedWeightScale, \
    const at::TensorList &gmm2Weight, \
    const at::TensorList &gmm2WeightScale, \
    const at::Tensor &expertScales, \
    const c10::optional<at::Tensor> &expertSmoothScales, \
    const c10::optional<at::Tensor> &xActiveMask, \
    c10::string_view groupEp, \
    int64_t epRankSize, \
    int64_t epRankId, \
    int64_t moeExpertNum, \
    int64_t sharedExpertNum, \
    int64_t sharedExpertRankNum, \
    int64_t quantMode, \
    int64_t globalBs
)
{
    auto xShape = x.sizes();
    auto expertIdsShape = expertIds.sizes();
    int h = xShape[1];
    int bs = xShape[0];
    int topk = expertIdsShape[1];
    
    at::Tensor output = at::empty({bs, h}, x.options());
    
    bool isShareExpert = (epRankId < sharedExpertRankNum);
    int64_t localExpertNum = 0;
    if (isShareExpert) {
        localExpertNum = 1;
    } else {
        localExpertNum = moeExpertNum / (epRankSize - sharedExpertRankNum);
    }
    auto opts = expertIds.options().dtype(at::kLong);
    at::Tensor expertTokenNums = at::empty({localExpertNum}, opts);
    
    // 必须要求对齐fused_deep_moe.cpp 先input 跟着 attr， 然后output
    vector<char> groupEpChrs(groupEp.begin(), groupEp.end());
    groupEpChrs.push_back('\0');
    char *groupEpPtr = &groupEpChrs[0];

    // 必须要求对齐fused_deep_moe.cpp 先input 跟着 attr, 然后output
    EXEC_NPU_CMD(aclnnFusedDeepMoe,
        // input
        x, expertIds, gmm1PermutedWeight, gmm1PermutedWeightScale, gmm2Weight, gmm2WeightScale, \
        expertScales, expertSmoothScales, xActiveMask, \
        // attr
        groupEpPtr, epRankSize, epRankId, moeExpertNum, sharedExpertNum, sharedExpertRankNum, quantMode, globalBs, \
        // output
        output, expertTokenNums);
    return {output, expertTokenNums};
}

TensorVector FusedDeepMoeBackwardImplNpu(const at::Tensor &self)
{
    at::Tensor result = at::Tensor(self); // 创建输出内存
    return {result, result};
}

TensorVector FusedDeepMoeImplMeta(
    const at::Tensor &x, \
    const at::Tensor &expertIds, \
    const at::TensorList &gmm1PermutedWeight, \
    const at::TensorList &gmm1PermutedWeightScale, \
    const at::TensorList &gmm2Weight, \
    const at::TensorList &gmm2WeightScale, \
    const at::Tensor &expertScales, \
    const c10::optional<at::Tensor> &expertSmoothScales, \
    const c10::optional<at::Tensor> &xActiveMask, \
    c10::string_view groupEp, \
    int64_t epRankSize, \
    int64_t epRankId, \
    int64_t moeExpertNum, \
    int64_t sharedExpertNum, \
    int64_t sharedExpertRankNum, \
    int64_t quantMode, \
    int64_t globalBs)
{
    auto xShape = x.sizes();
    int h = xShape[1];
    int bs = xShape[0];
    at::Tensor output = at::empty({bs, h}, x.options().device(at::kMeta));

    bool isShareExpert = (epRankId < sharedExpertRankNum);
    int64_t localExpertNum = 0;
    if (isShareExpert) {
        localExpertNum = 1;
    } else {
        localExpertNum = moeExpertNum / (epRankSize - sharedExpertRankNum);
    }
    auto opts = expertIds.options().dtype(at::kLong); 
    at::Tensor expertTokenNums = at::empty({localExpertNum}, opts.device(at::kMeta)); 
    
    return {output, expertTokenNums};
}

TensorVector FusedDeepMoeImpl(
    const at::Tensor &x, \
    const at::Tensor &expertIds, \
    const at::TensorList &gmm1PermutedWeight, \
    const at::TensorList &gmm1PermutedWeightScale, \
    const at::TensorList &gmm2Weight, \
    const at::TensorList &gmm2WeightScale, \
    const at::Tensor &expertScales, \
    const c10::optional<at::Tensor> &expertSmoothScales, \
    const c10::optional<at::Tensor> &xActiveMask, \
    c10::string_view groupEp, \
    int64_t epRankSize, \
    int64_t epRankId, \
    int64_t moeExpertNum, \
    int64_t sharedExpertNum, \
    int64_t sharedExpertRankNum, \
    int64_t quantMode, \
    int64_t globalBs)
{
    static auto op = torch::Dispatcher::singleton()
                        .findSchemaOrThrow("umdk_cam_op_lib::fused_deep_moe", "")
                        .typed<decltype(FusedDeepMoeImpl)>();
    return op.call(x, expertIds, gmm1PermutedWeight, gmm1PermutedWeightScale, gmm2Weight, gmm2WeightScale, \
        expertScales, expertSmoothScales, xActiveMask, \
        groupEp, epRankSize, epRankId, moeExpertNum, sharedExpertNum, sharedExpertRankNum, quantMode, globalBs);
}

// 通过继承torch::autograd::Function类实现前反向绑定
class ExtFusedDeepMoe : public torch::autograd::Function<ExtFusedDeepMoe> {
public:
    static TensorVector forward(AutogradContext *ctx, \
                            const at::Tensor &x, \
                            const at::Tensor &expertIds, \
                            const at::TensorList &gmm1PermutedWeight, \
                            const at::TensorList &gmm1PermutedWeightScale, \
                            const at::TensorList &gmm2Weight, \
                            const at::TensorList &gmm2WeightScale, \
                            const at::Tensor &expertScales, \
                            const c10::optional<at::Tensor> &expertSmoothScales, \
                            const c10::optional<at::Tensor> &xActiveMask, \
                            c10::string_view groupEp, \
                            int64_t epRankSize, \
                            int64_t epRankId, \
                            int64_t moeExpertNum, \
                            int64_t sharedExpertNum, \
                            int64_t sharedExpertRankNum, \
                            int64_t quantMode, \
                            int64_t globalBs)
    {
        auto result = FusedDeepMoeImpl(x, expertIds, gmm1PermutedWeight, gmm1PermutedWeightScale, gmm2Weight, \
            gmm2WeightScale, expertScales, expertSmoothScales, xActiveMask, \
            groupEp, epRankSize, epRankId, moeExpertNum, sharedExpertNum, sharedExpertRankNum, quantMode, globalBs);
        return result;
    }

    static TensorVector backward(AutogradContext *ctx, TensorVector grad_outputs)
    {
        return {at::Tensor(),
                at::Tensor(),
                at::Tensor(),
                at::Tensor(),
                at::Tensor(),
                at::Tensor(),
                at::Tensor(),
                at::Tensor(),
                at::Tensor(),
                at::Tensor()};
    }
};

TensorVector FusedDeepMoeImplAutograd(
    const at::Tensor &x, \
    const at::Tensor &expertIds, \
    const at::TensorList &gmm1PermutedWeight, \
    const at::TensorList &gmm1PermutedWeightScale, \
    const at::TensorList &gmm2Weight, \
    const at::TensorList &gmm2WeightScale, \
    const at::Tensor &expertScales, \
    const c10::optional<at::Tensor> &expertSmoothScales, \
    const c10::optional<at::Tensor> &xActiveMask, \
    c10::string_view groupEp, \
    int64_t epRankSize, \
    int64_t epRankId, \
    int64_t moeExpertNum, \
    int64_t sharedExpertNum, \
    int64_t sharedExpertRankNum, \
    int64_t quantMode, \
    int64_t globalBs)
{
    auto result = ExtFusedDeepMoe::apply(x, expertIds, gmm1PermutedWeight, gmm1PermutedWeightScale, gmm2Weight, \
            gmm2WeightScale, expertScales, expertSmoothScales, xActiveMask, \
            groupEp, epRankSize, epRankId, moeExpertNum, sharedExpertNum, sharedExpertRankNum, quantMode, globalBs);
        return result;
}

// fused_deep_moe
TORCH_LIBRARY_IMPL(umdk_cam_op_lib, PrivateUse1, m)
{
    m.impl("fused_deep_moe", &FusedDeepMoeImplNpu);
    m.impl("fused_deep_moe_backward", &FusedDeepMoeBackwardImplNpu);
}

TORCH_LIBRARY_IMPL(umdk_cam_op_lib, AutogradPrivateUse1, m)
{
    m.impl("fused_deep_moe", &FusedDeepMoeImplAutograd);
}

// 为Meta设备注册前反向实现
TORCH_LIBRARY_IMPL(umdk_cam_op_lib, Meta, m)
{
    m.impl("fused_deep_moe", &FusedDeepMoeImplMeta);
}